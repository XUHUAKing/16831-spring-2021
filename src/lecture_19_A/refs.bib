@article{littman2015reinforcement,
  title={Reinforcement learning improves behaviour from evaluative feedback},
  author={Littman, Michael L},
  journal={Nature},
  volume={521},
  number={7553},
  pages={445--451},
  year={2015},
  publisher={Nature Publishing Group}
}

@techreport{novikoff1963convergence,
  title={On convergence proofs for perceptrons},
  author={Novikoff, Albert B},
  year={1963},
  institution={Stanford Research Inst Menlo Park CA}
}


@book{connell_robot_2012,
	title = {Robot {Learning}},
	isbn = {978-1-4615-3184-5},
	abstract = {Building a robot that learns to perform a task has been acknowledged as one of the major challenges facing artificial intelligence. Self-improving robots would relieve humans from much of the drudgery of programming and would potentially allow operation in environments that were changeable or only partially known. Progress towards this goal would also make fundamental contributions to artificial intelligence by furthering our understanding of how to successfully integrate disparate abilities such as perception, planning, learning and action. Although its roots can be traced back to the late fifties, the area of robot learning has lately seen a resurgence of interest. The flurry of interest in robot learning has partly been fueled by exciting new work in the areas of reinforcement earning, behavior-based architectures, genetic algorithms, neural networks and the study of artificial life. Robot Learning gives an overview of some of the current research projects in robot learning being carried out at leading universities and research laboratories in the United States. The main research directions in robot learning covered in this book include: reinforcement learning, behavior-based architectures, neural networks, map learning, action models, navigation and guided exploration.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Connell, J. H. and Mahadevan, Sridhar},
	month = dec,
	year = {2012},
	note = {Google-Books-ID: 11DVBwAAQBAJ},
	keywords = {Computers / Intelligence (AI) \& Semantics, Technology \& Engineering / Automation, Technology \& Engineering / Manufacturing, Technology \& Engineering / Robotics}
}


@book{sutton_reinforcement_1998,
	address = {Cambridge, Mass},
	series = {Adaptive computation and machine learning},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-19398-6},
	shorttitle = {Reinforcement learning},
	language = {en},
	publisher = {MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {1998},
	keywords = {Reinforcement learning},
	file = {Sutton and Barto - 1998 - Reinforcement learning an introduction.pdf:C\:\\Users\\Scott\\Zotero\\storage\\7HSQ2GXR\\Sutton and Barto - 1998 - Reinforcement learning an introduction.pdf:application/pdf}
}

@inproceedings{lu2010contextual,
  title={Contextual multi-armed bandits},
  author={Lu, Tyler and P{\'a}l, D{\'a}vid and P{\'a}l, Martin},
  booktitle={Proceedings of the Thirteenth international conference on Artificial Intelligence and Statistics},
  pages={485--492},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}


@article{singh_reinforcement_1996,
	title = {Reinforcement learning with replacing eligibility traces},
	volume = {22},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00114726},
	doi = {10.1007/BF00114726},
	abstract = {The eligibility trace is one of the basic mechanisms used in reinforcement learning to handle delayed reward. In this paper we introduce a new kind of eligibility trace, thereplacing trace, analyze it theoretically, and show that it results in faster, more reliable learning than the conventional trace. Both kinds of trace assign credit to prior events according to how recently they occurred, but only the conventional trace gives greater credit to repeated events. Our analysis is for conventional and replace-trace versions of the offline TD(1) algorithm applied to undiscounted absorbing Markov chains. First, we show that these methods converge under repeated presentations of the training set to the same predictions as two well known Monte Carlo methods. We then analyze the relative efficiency of the two Monte Carlo methods. We show that the method corresponding to conventional TD is biased, whereas the method corresponding to replace-trace TD is unbiased. In addition, we show that the method corresponding to replacing traces is closely related to the maximum likelihood solution for these tasks, and that its mean squared error is always lower in the long run. Computational results confirm these analyses and show that they are applicable more generally. In particular, we show that replacing traces significantly improve performance and reduce parameter sensitivity on the "Mountain-Car" task, a full reinforcement-learning problem with a continuous state space, when using a feature-based function approximator.},
	language = {en},
	number = {1},
	urldate = {2021-04-17},
	journal = {Machine Learning},
	author = {Singh, Satinder P. and Sutton, Richard S.},
	month = mar,
	year = {1996},
	pages = {123--158},
	file = {Springer Full Text PDF:C\:\\Users\\Zongyue\\Zotero\\storage\\ERV6ABMT\\Singh and Sutton - 1996 - Reinforcement learning with replacing eligibility .pdf:application/pdf}
}


@article{whittle1980multi,
  title={Multi-armed bandits and the Gittins index},
  author={Whittle, Peter},
  journal={Journal of the Royal Statistical Society: Series B (Methodological)},
  volume={42},
  number={2},
  pages={143--149},
  year={1980},
  publisher={Wiley Online Library}
}

@article{vovk1998game,
  title={A game of prediction with expert advice},
  author={Vovk, Vladimir},
  journal={Journal of Computer and System Sciences},
  volume={56},
  number={2},
  pages={153--173},
  year={1998},
  publisher={Elsevier}
}

@article{ohta1984reachability,
  title={Reachability, observability, and realizability of continuous-time positive systems},
  author={Ohta, Yoshito and Maeda, Hajime and Kodama, Shinzo},
  journal={SIAM journal on control and optimization},
  volume={22},
  number={2},
  pages={171--180},
  year={1984},
  publisher={SIAM}
}

@inproceedings{super,
  title={An empirical comparison of supervised learning algorithms},
  author={Caruana, Rich and Niculescu-Mizil, Alexandru},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={161--168},
  year={2006}
}