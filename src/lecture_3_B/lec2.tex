\documentclass[11pt]{article}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{listings}[language=Python]
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{epsfig}
\usepackage[tight]{subfigure}
\usepackage{xcolor}

\DeclareMathOperator*{\minimize}{min}
\DeclareMathOperator*{\maximize}{max}
\newcommand{\HH}{\mathcal{H}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\V}{\mathbf{V}}

\usepackage{algorithm}
 %on linux you may need to run sudo apt-get install texlive-full to install algorithm.sys
\usepackage{algorithmic}

\usepackage{verbatim}

\newcommand{\handout}[5]{
  \noindent
  \begin{center}
  \framebox{
    \vbox{
      \hbox to 5.78in { {#1} \hfill #2 }
      \vspace{4mm}
      \hbox to 5.78in { {\Large \hfill #5  \hfill} }
      \vspace{2mm}
      \hbox to 5.78in { {\em #3 \hfill #4} }
    }
  }
  \end{center}
  \vspace*{4mm}
}

\newcommand{\lecture}[5]{\handout{#1}{#2}{#3}{#4}{#5}}
\newcommand{\collision}[0]{\mathrm{collision}}
\newcommand{\nocollision}[0]{\overline{\collision}}
\newcommand*{\QED}{\hfill\ensuremath{\square}}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{note}[theorem]{Note}

% 1-inch margins, from fullpage.sty by H.Partl, Version 2, Dec. 15, 1988.
\topmargin 0pt
\advance \topmargin by -\headheight
\advance \topmargin by -\headsep
\textheight 8.9in
\oddsidemargin 0pt
\evensidemargin \oddsidemargin
\marginparwidth 0.5in
\textwidth 6.5in

\parindent 0in
\parskip 1.5ex
%\renewcommand{\baselinestretch}{1.25}

\begin{document}
\definecolor{ultramarine}{RGB}{0,120,200}
\definecolor{purple}{RGB}{130,30,200}

% $$\texttt{Performance}_{\texttt{SOTA}} = \sum_{s, a} \Big( \textcolor{ultramarine}{\mathbf{\mathcal{R}_{expert}} (s, a) - \textcolor{purple}{\mathbf{\mathcal{R}_{SOTA}} (s, a)}} \Big)^2$$

\lecture{Statistical Techniques in Robotics (16-831, S20)}{Lecture \#3
  (Wednesday, February 8)}{Lecturer: Kris Kitani}{Scribes: Zhengyi Luo, Erica Weng}{Halving Algorithm, Randomized Greedy Algorithm, Regret}
\
%This section serves as a review of the previous lecture and any other context required to frame the content of the current lecture. 

%You may format the scribes in any way you like, aside from changing font style, size and page format. Please use subsections and paragraphs to increase the readability of your notes.

%Length requirement 1-2 pages.


% \textbf{Things for Zen include to include}
% \begin{enumerate}
%     \item statistical vs. online learning: iid assumption vs no assumption
%     \item review greedy algo and its mistake bound
%     \item define iid
%     \item define adversary 
%     \item regret is a mistake bound
%     \item include regret definition with equations
% \end{enumerate}

\section{Recap}
In the previous lecture, we discussed one algorithm for the problem of Prediction With Expert Advice (PWEA) -- the Consistent Algorithm. PWEA is a problem of online learning where the system takes decisions based on advice from a set of experts over time with the goal to be as good as possible as the best expert in the group, if not better. Over time, the system learns from making mistakes based on the advice from each expert and updates the hypothesis function which modifies the weighting given to each expert in the group. The PWEA problem is one-shot, instructive, and exhaustive. The different ways to choose your hypothesis function and weight update strategy give us different variations of the Consistent Algorithm.\cite{Bianchi1997Expert,Bianchi1999sequences}

\section{Online Learning}
We formally define online learning and differentiate online learning from statistical learning (e.g., supervised learning). Specifically, statistical learning assumes that the input data consist of i.i.d. (independent and identically distributed) random variables. Namely, each input data/random variable has the same probability distribution as the others and are all mutually independent. In online learning, we make no such assumption, and the input data comes from a stream and the distribution of the data can change over time. The data samples are not identically distributed and can even be generated by an adversarial agent. 

\subsection{Adversarial agent}
In online learning, the input sequential data can be generated by an ``adversarial agent" or ``nature" that is working against our algorithm. For instance, assume a game of questions answering where the learner's objective is to learn from a sequence of questions and answers provided by the host. The host can deliberately design questions that have conflicting answers such that the learner can never learn from his/her mistakes and can never win. It is possible that with proper design the questions can never be answered perfectly. On the other hand, if we assume realizability, then there exists a perfect mapping from the question to the correct answer. 

\section{Predicting with Expert Advice Problem Setup}
In this lecture, we continue to study the online learning problem called "Predicting with Expert Advice Problem Setup". The problem is identical to that used in the last lecture. Concisely, we have: the instance domain $\X$, or the set of possible observations that can be seen at any timestep $t$, which is the information/advice from experts provided to us for making a decision. We have the target domain $\Y$, also known as the output space, which is the set of possible outcomes that can occur. We have a hypothesis class $\HH = \{h: \X \rightarrow \Y \}$, a series of functions mapping an element in the instance domain to an element in the target domain, each of which determines how we will make a prediction based on the observations we see. The Version space $\V_t$ at time $t$ is defined as the set of experts whom we still trust at time $t$; all experts in $V_t$ have not yet made a mistake at time $t$. We assume realizability (learner has access to the perfect hypothesis) for both algorithms described in these notes.

\section{Halving Algorithm}
The Halving algorithm improves the mistake bound of the Consistent Algorithm significantly. At each timestep $t$, instead of picking the first expert from the pool of experts who have been consistently correct in their predictions until $t$, we will pick the decision which has at least $\frac{1}{|\Y|}$ expert representation ($|\Y| = 2$ in the PWEA problem). By pigeonhole principle, at least one choice $\{y\in \Y\}$ will have at least $\frac{1}{|\Y|}$ representation among the experts' advice. With each mistake made, the size of the version space (the pool of consistent experts) reduces by at least $\frac{1}{|\Y|}$; that is, $|\V^{(t+1)}| \le \frac{|\V^{(t)}|}{|\Y|}$.\cite{slides} The Halving Algorithm pseudocode is listed in algorithm \ref{algo:halving}.

\begin{algorithm}[H]
\caption{Halving Algorithm}
\label{algo:halving}
\begin{algorithmic}[1]
\STATE $\mathbf{V^{(1)}} = \mathcal{H}$  \hfill $\triangleright$ Version space initially stores all hypotheses
\FOR{$t=1,\;\cdots,\;T$}
\STATE \textsc{Receive} ($\textbf{x}^{(t)}$) \hfill $\triangleright$ Receive expert prediction
\STATE $h =\textsc{Majority}(\textbf{V}^{(t)}) \hfill \triangleright$ Select majority hypothesis 
\STATE $\hat{y}^{(t)} = h(\mathbf{x}^{(t)})$ \hfill $\triangleright$ Rredict using selected hypothesis
\STATE \textsc{Receive} ($y^{(t)}$) \hfill $\triangleright$ Receive true outcome
\STATE $\mathbf{V}^{(t+1)}\leftarrow \{ h \in \mathbf{V}^{(t)} : h(\textbf{x})^{(t)}=y^{(t)} \}$ \hfill $\triangleright$ Update Version space, removing inconsistent experts
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Worst-case Mistake Bounds for the Halving Algorithm}

\theorem{\textbf{(Mistake bound of Halving Algorithm)} Let $M_H^{(t)}(\mathcal{H})$ be the total number of mistakes made by the Halving Algorithm by timestep $t$ for hypothesis class $\HH$. Assuming realizability and that there is at least one expert in the starting Version space $\V$, $M_H^{(T)}(\mathcal{H})$ is upper-bounded as:
$$M_H^{(T)}(\mathcal{H}) \le \log_2|\mathcal{H}| $$
\label{theorem:greedy}}
\proof{We use induction to establish an upper bound on the size of the Version space at time $t$.\\Initially, there are $|\HH|$ experts, so our base case is: $$|\mathbf{V}^{(1)}| = |\HH|}$$
Now our inductive step. If the algorithm makes a mistake at time $t$, then:
$$|\mathbf{V}^{(t+1)}| \le \frac{1}{2} \cdot |\mathbf{V}^{(t)}|}$$
Because the algorithm picks the highest-represented decision at each time $t$, if the algorithm makes a mistake on time $t$, we know at most $\frac{1}{2}$ experts followed the correct decision but no more. Eliminating the incorrect experts from the Version space, we are left with at most $\frac{1}{2}$ of the remaining at $t+1$. So after $M_H^{(t)}$ mistakes, at step $t$:
$$|\mathbf{V}^{(t)}| \le \Bigg(\frac{1}{2}\Bigg)^{M_{H}^{(t)}} \cdot |\mathcal{H}|$$
This is an upper bound on the size of the version space at time t.
\\For the lower bound: since we assume realizability, then there is at least one expert at any time t:
$$|\mathbf{V}^{(t)}| \ge  1$$ 
Putting the upper and lower bounds together, we get:
$$1 \le \Bigg(\frac{1}{2}\Bigg)^{M_{H}^{(t)}} \cdot |\mathcal{H}| $$ 
Simplifying:
$$2^{M_{H}^{(t)}} \le |\mathcal{H}|$$
$$M_{H}^{(t)} \le \log_{2}|\mathcal{H}|$$
\QED}

\section{Randomized Greedy Algorithm}
The randomized greedy algorithm takes another step toward better mistake bounds, thwarting the adversary by picking hypotheses through uniform sampling. At each time step, the algorithm chooses a random expert that has not yet made a mistake by random and follows its advice. The algorithm is identical to the greedy algorithm, save that the expert is chosen at random from the Version space, as opposed to simply picking the first expert in the Version space. This minor change leads to a significant improvement in mistake bounds in adversarial analysis. \cite{slides} The Randomized Greedy Algorithm pseudocode is listed in algorithm \ref{algo:random}.

\begin{algorithm}[H]
\caption{Randomized Greedy Algorithm}
\label{algo:random}
\begin{algorithmic}[1]
\STATE $\mathbf{V^{(1)}} = \mathcal{H}$  \hfill $\triangleright$ Version space initially stores all hypotheses
\FOR{$t=1,\;\cdots,\;T$}
\STATE \textsc{Receive} ($\textbf{x}^{(t)}$) \hfill $\triangleright$ Receive expert prediction
\STATE $h =\textsc{Random}(\textbf{V}^{(t)}) \hfill \triangleright$ Random selection from Version space
\STATE $\hat{y}^{(t)} = h(\mathbf{x}^{(t)})$ \hfill $\triangleright$ predict using selected expert
\STATE \textsc{Receive} ($y^{(t)}$) \hfill $\triangleright$ Receive true outcome
\STATE $\mathbf{V}^{(t+1)}\leftarrow \{ h \in \mathbf{V}^{(t)} : h(\textbf{x})^{(t)}=y^{(t)} \}$ \hfill $\triangleright$ Update Version space, removing inconsistent experts
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Worst-case Mistake Bounds for the Randomized Greedy Algorithm}

\theorem{\textbf{(Mistake bound of Randomized Greedy Algorithm}) Let $M_{RG}^{(t)}(\mathcal{H})$ be the total number of mistakes made by the Randomized Greedy Algorithm by timestep $t$ for hypothesis class $\HH$. Assuming realizability and that there is at least one expert in the starting Version space $\V^{(1)}$, $M_{RG}^{(T)}(\mathcal{H})$ is upper-bounded as:
$$M_{RG}^{(t)}(\mathcal{H}) \le \ln|\mathcal{H}|$$
\label{theorem:greedy}}
\proof{We use induction to establish an upper bound on the size of the Version space at time $t$.\\
Initially, there are $|\HH|$ experts, so our base case is: $$|\V^{(1)}| = |\HH|$$
Now our inductive step. If the algorithm makes a mistake at time $t$, then:
% $$E\Big[|\V^{(t+1)}|\Big] \le \cdot |\V^{(t)}|}$$
$$|\V^{(t+1)}| = \alpha^{(t)}|\V^{(t)}|}$$ 
where $\alpha^{(t)}$ is the factor our Version space decreases by each time step due to eliminating inconsistent experts -- in other words, the ``probability" any one expert will be correct on timestep $T$. Since our algorithm is randomized, even in the worst case $\alpha$ will vary from timestep to timestep. After $T$ timesteps,
$$|\V^{T}| = |\HH| \cdot \prod_{t=1}^T \alpha^{(t)}$$ 

We use the useful mathematical inequality $x \le e^{-(x-1)}$ for real $x$ and simplifying:
$$|\V^{T}| \le |\HH| \cdot \prod_{t=1}^T e^{-(1-\alpha^{(t)})}$$ 
$$|\V^{T}| \le|\HH| \cdot  e^{-\sum_{t=1}^T(1-\alpha^{(t)})}$$ 
Note that $1-\alpha^{(t)}$ is proportion of experts who made a mistake at timestep $t$. thus, $\sum_{t=1}^T(1-\alpha^{(t)})}$ is the total number of mistakes that have been made from $t=1\cdots T$. That is, $M_{RG}^{(T)}$, the metric we want to bound.

For the lower bound: since we assume realizability, then there is at least one expert at any time t:
$$|\mathbf{V}^{(t)}| \ge  1$$ 
Putting the upper and lower bounds together, we get:
$$1 \le \HH \cdot e^ {-M_{RG}^{(T)}}$$

Rearranging and simplifying:
$$M_{RG}^{(t)} \le \ln|\HH}|$$
\QED}

\section{Regret}
Up till now, our algorithms and analysis are all carried out under the umbrella of assuming realizability--namely, there exists a perfect hypothesis that makes no mistake. Thus, so far we have evaluated our algorithms using the mistake bounds, which tell us how far away our online algorithm is from a perfect hypothesis that makes 0 mistakes. However, if there is no such perfect hypothesis, the mistake bound is no longer a valid evaluation metric (because even the best hypothesis will make mistakes). To measure the performance gap of the ``best" (maybe not perfect) hypothesis and our online learning algorithm, we introduce regret.\\

Formerly, in a hypothesis class $\mathcal{H}$, if we relax the realizability assumption (no longer assume the perfect hypothesis $h^{\star}$ exist), we want to measure the difference between our online algorithm $\ell$ and any hypothesis in $h \in \mathcal{H}$. The regret $R^{T}(h)$ over time period T between $\ell$ and $h$ is defined as the difference between the cumulative loss of  $\ell$ and $h$ : 
$$R^{(T)}(h)=\sum_{t=1}^{T} \ell\left(\hat{y}^{(t)}, y^{(t)}\right)-\sum_{t=1}^{T} \ell\left(h\left(\boldsymbol{x}^{(t)}\right), y^{(t)}\right)$$

The regret between our learner and the best hypothesis (that achieves the cumulative minimal loss) is defined as follows: 
$$R^{(T)}(\mathcal{H}) = \max _{h} R^{(T)}(h)=\sum_{t=1}^{T} \ell\left(\hat{y}^{(t)}, y^{(t)}\right)-\min _{h} \sum_{t=1}^{T} \ell\left(h\left(\boldsymbol{x}^{(t)}\right), y^{(t)}\right)$$

In online learning, we would like to find an algorithm whose average regret is bounded as $T$ goes to infinity: 
$$\begin{aligned}
\lim_{T\rightarrow \infty} \frac{1}{T} R^{(T)}(\mathcal{H}) &=\lim _{T \rightarrow \infty} \sum_{t=1}^{T} \ell^{(t)}\left(\hat{y}^{(t)}, y^{(t)}\right)-\min _{h \in \mathcal{H}} \ell^{(t)}\left(h\left(x^{(t)}\right), y^{(t)}\right) \\
&=0
\end{aligned}$$

$$\text{or}$$

$$\frac{R^{(T)}(\mathcal{H})}{T} \rightarrow 0 \text{ as } T \rightarrow \infty$$

If your average regret goes to $0$ as $T$ goes to infinity, you have a ``no regret " algorithm. 
For the average regret to be bounded, the regret must grow sub-linearly with respect to $T$. In the long run, a no regret algorithm performs no worse than the best hypothesis in hindsight.


\section{Conclusion}
In this lecture, we introduced two more algorithms to solve the PWEA problem, the Halving Algorithm and the Randomized Greedy Algorithm, both of which assumed the realizability. We derived the mistake bound for both of these algorithms, and both showed great improvement over the Consistent Alogrithm. 

We also formally introduced the online learning problem, and the setting online learning usually perform in -- assuming no realizability. Once we remove the realizability assumption, we can no longer use the mistake bound as our measure for comparing between the best hypothesis and our algorithm. Thus, we introduced regret as an alternative evaluation metric to compare between our learner and the best hypothesis. Assuming no realizability also means that we can no longer remove a hypothesis as soon as it makes a mistake (something all our aforementioned algorithms do).  In the next lecture, we will introduce new algorithms that can operate under no realizability assumption, and we will analyze these algorithms using the regret bound. 

% Its major flaws are that it requires the strong assumption of realizability, and uses relatively little domain information about the problem. In later lectures we will look at algorithms which slacken the assumptions and do better at incorporating knowledge into the algorithm.

%Include your references here. Please cite any resources you found useful.	
%Populate the refs.bib file or list your references manually. Be consistent in formatting!
{
\bibliography{refs}
\bibliographystyle{abbrv}
}

%\section{Appendix}
%This section provides any relevant background material that was not covered in the lectures, but was found to be useful for understanding the material. 
%For example, derivations, theory underlying techniques employed, etc. 

%Additionally, this section can summarizes applications or extensions of these techniques found in the literature. 

\end{document} % Done!


